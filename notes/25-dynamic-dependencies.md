## Dynamic Dependencies

Nix and Bazel don't allow dynamic dependencies. I think there is an argument to be made that this is the reason their ergonomics are so poor. Nix libs that are intended to build arbitrary projects in a given language rely heavily on code generation. Arguably this is a type of dynamic dependencies.

I think it would be interesting to explore first class support for dynamic dependencies in Bramble. Maybe if they are easy to use and set up we can limit the amount of derivations that need network access. If you can generate arbitrary calls to `fetch_url` within a derivation, then maybe you can get away with just that.

One Idea:

There is a specific type of derivation that outputs starlark code. It is a different color than regular derivations (so we can detect it statically), and only outputs starlark code. This starlark code is run once the derivation is done building. We would need to update the dependency graph as we build.

This has some weird implications because we would still need to reference the build output. Do we just need to ensure that the generated code just outputs a single derivation?

So let's think about numpy.

```python
def foo():
    pip_install("numpy")
```

There is no real way to do this because numpy will need to download its own dependencies. So we could either:

1. Download them within the derivations using the network, but then other depedencies might generate their own independent depedencies, which would be duplicated and might conflict.
2. Generate code for each dependency, which totally works, but dosn't have first class support.


```python
def foo():
    pip_install("numpy")

def pip_install(name):
    deps = fetch_url("dependency_finder.gov/"+name)
    derivation(script="""
    out = ""
    for dep in deps:
        out += "fetch_url(dep)\n"
    return out
    """)
```

Terrible pseudo-code, but basically this derivation returns starlark code with the deps we need to download.

If we go this route, we would need to be able to check if we've generated this derivation on the fly. I think if we don't do that, it would be very hard to do things like: validating current url hashes, without actually building.

We could just stick the outputted starlark into the store somewhere, but it might be better to generate code and keep it in the source of the project. If the interface is just like in the example above `fetch_url("numpy")` then what if a new version of numpy is published? Any time there was a rebuild the url hash would mismatch. I think ideally if we want to replicate something like a Cargo.lock we would need the output of the code generating derivation to be placed within the project tree. That way, that generated file could reference very specific versions of software to fetch. If the end user wanted to fetch a new version they would simply delete the generated file.

This doesn't really remove the need for derivations that access the network. The code generating derivation would still need to make a request for the `numpy` source in order to calculate dependencies.

This kind of thing would mean that you could truly write a derivation like `pip_install("numpy")` without code generation that would require certain setup.

We could also prevent code generated by a code generating derivation from calling another code generating derivation, at least at the start, to limit all kinds gnarly behavior.
